{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capstone Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monicman/dataengineering/blob/master/Capstone_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH7xBA0fUNiP",
        "colab_type": "code",
        "outputId": "cff4390b-0c61-4e08-823b-df66e1dff5c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (2.4.4)\n",
            "Requirement already satisfied: py4j==0.10.7 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nVeXATOaqpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkFiles\n",
        "import urllib\n",
        "from urllib.request import urlopen   \n",
        "import io\n",
        "import requests\n",
        "from zipfile import ZipFile\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gkz03z6xjlDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "def ingest_files(url,filename):\n",
        "  spark.sparkContext.addFile(url)\n",
        "  if url[-3:] == 'csv':\n",
        "    return spark.read.csv(SparkFiles.get(filename), header=True)\n",
        "  elif url[-3:] == 'zip':\n",
        "    url = urlopen(url)\n",
        "    output = open(filename, 'wb')    # note the flag:  \"wb\"        \n",
        "    output.write(url.read())\n",
        "    output.close()\n",
        "    print(\"error here\")\n",
        "    df = pd.read_csv(filename,sep='\\t')\n",
        "    print(\"no actually here\")\n",
        "    return spark.createDataFrame(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LE2dMeei3MQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spark = SparkSession.builder.getOrCreate()\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType\n",
        "\n",
        "def ingest_files(url,filename):\n",
        "  spark.sparkContext.addFile(url)\n",
        "  if url[-3:] == 'csv':\n",
        "    return spark.read.csv(SparkFiles.get(filename), header=True)\n",
        "  elif url[-3:] == 'zip':\n",
        "    url = urlopen(url)\n",
        "    output = open(filename, 'wb')    # note the flag:  \"wb\"        \n",
        "    output.write(url.read())\n",
        "    output.close()\n",
        "    df = pd.read_csv(filename,sep='\\t')\n",
        "    df.iloc[:, 3:6] = df.iloc[:, 3:6].apply(pd.to_datetime, errors='coerce', format='%d/%m/%Y')\n",
        "    myschema = StructType([StructField('REGISTER_NAME', StringType(), True),\n",
        "                         StructField('BN_NAME', StringType(), True),\n",
        "                         StructField('BN_STATUS', StringType(), True),\n",
        "                         StructField('BN_REG_DT', DateType(), True),\n",
        "                         StructField('BN_CANCEL_DT', DateType(), True),\n",
        "                         StructField('BN_RENEW_DT', DateType(), True),\n",
        "                         StructField('BN_STATE_NUM', StringType(), True),\n",
        "                         StructField('BN_STATE_OF_REG', StringType(), True),\n",
        "                         StructField('BN_ABN', DoubleType(), True)])\n",
        "    return spark.createDataFrame(df, schema=myschema)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtvyMIRnZdVq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "fdafd67e-9991-4ef6-c370-a6f669f7e872"
      },
      "source": [
        "ipgod101 = 'https://data.gov.au/data/dataset/a4210de2-9cbb-4d43-848d-46138fefd271/resource/e5cbeafc-5fb3-4dfd-bd22-afe81b6ab1e1/download/ipgod101.csv'\n",
        "ipgod102 = 'https://data.gov.au/data/dataset/a4210de2-9cbb-4d43-848d-46138fefd271/resource/846990df-db42-4ad7-bbd6-567fd37a2797/download/ipgod102.csv'  \n",
        "ABN = 'https://data.gov.au/data/dataset/bc515135-4bb6-4d50-957a-3713709a76d3/resource/839cc783-876f-47a2-a70c-0fe606977517/download/business_names_201909.zip'\n",
        "\n",
        "#staging_ipgod101 = ingest_files(ipgod101,'ipgod101.csv')\n",
        "#staging_ipgod102 = ingest_files(ipgod102,'ipgod102.csv')\n",
        "staging_abr = ingest_files(ABN,'zipFile.zip')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-138e9af75fdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#staging_ipgod101 = ingest_files(ipgod101,'ipgod101.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#staging_ipgod102 = ingest_files(ipgod102,'ipgod102.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mstaging_abr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mingest_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mABN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'zipFile.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-515a458afed3>\u001b[0m in \u001b[0;36mingest_files\u001b[0;34m(url, filename)\u001b[0m\n\u001b[1;32m     22\u001b[0m                          \u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BN_STATE_OF_REG'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                          StructField('BN_ABN', IntegerType(), True)])\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmyschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    746\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;31m# make sure data could consumed multiple times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mverify_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36mverify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1387\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mverify_nullability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m             \u001b[0mverify_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36mverify_struct\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1368\u001b[0m                                 \"length of fields (%d)\" % (len(obj), len(verifiers))))\n\u001b[1;32m   1369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifier\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifiers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m                     \u001b[0mverifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__dict__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m                 \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36mverify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1387\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mverify_nullability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m             \u001b[0mverify_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36mverify_integer\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mverify_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m             \u001b[0massert_acceptable_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m             \u001b[0mverify_acceptable_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1316\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2147483648\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2147483647\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36mverify_acceptable_types\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_acceptable_types\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m             raise TypeError(new_msg(\"%s can not accept object %r in type %s\"\n\u001b[0;32m-> 1278\u001b[0;31m                                     % (dataType, obj, type(obj))))\n\u001b[0m\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: field BN_ABN: IntegerType can not accept object 27474223588.0 in type <class 'float'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nttHHqhaYLo",
        "colab_type": "code",
        "outputId": "96e37b95-99c2-4fb8-913b-c2165f63d318",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "ABN = 'https://data.gov.au/data/dataset/bc515135-4bb6-4d50-957a-3713709a76d3/resource/839cc783-876f-47a2-a70c-0fe606977517/download/business_names_201909.zip'\n",
        "staging_abr.head(10)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(REGISTER_NAME='BUSINESS NAMES', BN_NAME='         STEFAN ELECTRICAL TM.', BN_STATUS='Registered', BN_REG_DT=datetime.date(2014, 8, 6), BN_CANCEL_DT=None, BN_RENEW_DT=datetime.date(2020, 8, 6), BN_STATE_NUM='NaN', BN_STATE_OF_REG='NaN', BN_ABN=27474223588.0),\n",
              " Row(REGISTER_NAME='BUSINESS NAMES', BN_NAME='    Twinkle Toes Baby Hand and Feet Sculptures - North Melbourne', BN_STATUS='Registered', BN_REG_DT=datetime.date(2016, 8, 3), BN_CANCEL_DT=None, BN_RENEW_DT=datetime.date(2019, 6, 23), BN_STATE_NUM='NaN', BN_STATE_OF_REG='NaN', BN_ABN=29133895342.0),\n",
              " Row(REGISTER_NAME='BUSINESS NAMES', BN_NAME='   Nourishing Your World', BN_STATUS='Registered', BN_REG_DT=datetime.date(2017, 3, 13), BN_CANCEL_DT=None, BN_RENEW_DT=datetime.date(2020, 3, 13), BN_STATE_NUM='NaN', BN_STATE_OF_REG='NaN', BN_ABN=23850847348.0),\n",
              " Row(REGISTER_NAME='BUSINESS NAMES', BN_NAME='  colour expert painting and decorating service', BN_STATUS='Registered', BN_REG_DT=datetime.date(2013, 4, 23), BN_CANCEL_DT=None, BN_RENEW_DT=datetime.date(2022, 4, 23), BN_STATE_NUM='NaN', BN_STATE_OF_REG='NaN', BN_ABN=27273870602.0),\n",
              " Row(REGISTER_NAME='BUSINESS NAMES', BN_NAME='  KARINYA FARM CAPE SCHANCK', BN_STATUS='Registered', BN_REG_DT=datetime.date(2018, 9, 6), BN_CANCEL_DT=None, BN_RENEW_DT=datetime.date(2021, 9, 6), BN_STATE_NUM='NaN', BN_STATE_OF_REG='NaN', BN_ABN=85342032664.0),\n",
              " Row(REGISTER_NAME='BUSINESS NAMES', BN_NAME='  Melbourne Star Painting and Decorating', BN_STATUS='Registered', BN_REG_DT=datetime.date(2017, 3, 7), BN_CANCEL_DT=None, BN_RENEW_DT=datetime.date(2020, 3, 7), BN_STATE_NUM='NaN', BN_STATE_OF_REG='NaN', BN_ABN=39512379135.0),\n",
              " Row(REGISTER_NAME='BUSINESS NAMES', BN_NAME='  MINH DUC TRAN CLOTHING', BN_STATUS='Registered', BN_REG_DT=datetime.date(2003, 7, 2), BN_CANCEL_DT=None, BN_RENEW_DT=datetime.date(2021, 7, 2), BN_STATE_NUM='B1698762E', BN_STATE_OF_REG='VIC', BN_ABN=nan),\n",
              " Row(REGISTER_NAME='BUSINESS NAMES', BN_NAME='  OSCAR REAL ESTATE', BN_STATUS='Registered', BN_REG_DT=datetime.date(2013, 1, 22), BN_CANCEL_DT=None, BN_RENEW_DT=datetime.date(2022, 1, 22), BN_STATE_NUM='NaN', BN_STATE_OF_REG='NaN', BN_ABN=99161953662.0),\n",
              " Row(REGISTER_NAME='BUSINESS NAMES', BN_NAME=\"  Surgeon's Cottage Morpeth\", BN_STATUS='Registered', BN_REG_DT=datetime.date(2013, 8, 9), BN_CANCEL_DT=None, BN_RENEW_DT=datetime.date(2020, 8, 9), BN_STATE_NUM='NaN', BN_STATE_OF_REG='NaN', BN_ABN=54681763546.0),\n",
              " Row(REGISTER_NAME='BUSINESS NAMES', BN_NAME='  Sutherland Shire Commercial Real Estate', BN_STATUS='Registered', BN_REG_DT=datetime.date(2018, 10, 26), BN_CANCEL_DT=None, BN_RENEW_DT=datetime.date(2022, 10, 26), BN_STATE_NUM='NaN', BN_STATE_OF_REG='NaN', BN_ABN=71259135597.0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdRqJX2SDQ8y",
        "colab_type": "code",
        "outputId": "bf0bcbd4-d7a3-4563-f1a1-4b65371ded96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "staging_ipgod101.head(5)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(australian_appl_no='2018904968', application_year='2018', sealing_year=None, applicant_count='1', australian='True', foreign='False', entity='False', non_entity='True', patent_status_type='FILED', patent_type='Provisional', primary_ipc_mark_value=None),\n",
              " Row(australian_appl_no='2017215797', application_year='2018', sealing_year=None, applicant_count='1', australian='False', foreign='True', entity='True', non_entity='False', patent_status_type='FILED', patent_type='Standard', primary_ipc_mark_value='E06B   9/00'),\n",
              " Row(australian_appl_no='2017216336', application_year='2018', sealing_year=None, applicant_count='1', australian='False', foreign='True', entity='True', non_entity='False', patent_status_type='FILED', patent_type='Standard', primary_ipc_mark_value='A61M  31/00'),\n",
              " Row(australian_appl_no='2017216324', application_year='2018', sealing_year=None, applicant_count='1', australian='False', foreign='True', entity='True', non_entity='False', patent_status_type='FILED', patent_type='Standard', primary_ipc_mark_value='H04W  74/08'),\n",
              " Row(australian_appl_no='2017216313', application_year='2018', sealing_year=None, applicant_count='1', australian='False', foreign='True', entity='True', non_entity='False', patent_status_type='FILED', patent_type='Standard', primary_ipc_mark_value='H01L  29/06')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xFAljD8DWLP",
        "colab_type": "code",
        "outputId": "fb7d7bb5-809d-4526-dfb8-59e2e4ca1eee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "staging_ipgod102.head(5)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(australian_appl_no='1904000001', ipa_id=None, country='ZZ', australian='False', entity='False', name='non-entity', cleanname='non-entity', corp_desg=None, state=None, postcode=None, lat=None, lon=None, sa2_code=None, sa2_name=None, lga_code=None, lga_name=None, gcc_name=None, elect_div=None, abn=None, acn=None),\n",
              " Row(australian_appl_no='1904000004', ipa_id=None, country='ZZ', australian='False', entity='False', name='non-entity', cleanname='non-entity', corp_desg=None, state=None, postcode=None, lat=None, lon=None, sa2_code=None, sa2_name=None, lga_code=None, lga_name=None, gcc_name=None, elect_div=None, abn=None, acn=None),\n",
              " Row(australian_appl_no='1904000006', ipa_id=None, country='ZZ', australian='False', entity='False', name='non-entity', cleanname='non-entity', corp_desg=None, state=None, postcode=None, lat=None, lon=None, sa2_code=None, sa2_name=None, lga_code=None, lga_name=None, gcc_name=None, elect_div=None, abn=None, acn=None),\n",
              " Row(australian_appl_no='1904000009', ipa_id=None, country='ZZ', australian='False', entity='True', name='andin what manner thesame ls Figure Iis a sectional front elevation of tobe performed tobe particularly described weighing apparatus involving mo invention. and ascertained in and bv the', cleanname='ANDIN WHAT MANNER THESAME LS FIGURE IIS A SECTIONAL FRONT ELEVATION OF TOBE PERFORMED TOBE PARTICULARLY DESCRIBED WEIGHING APPARATUS INVOLVING MO INVENTION & ASCERTAINED IN', corp_desg='BV', state=None, postcode=None, lat=None, lon=None, sa2_code=None, sa2_name=None, lga_code=None, lga_name=None, gcc_name=None, elect_div=None, abn=None, acn=None),\n",
              " Row(australian_appl_no='1904000010', ipa_id=None, country='ZZ', australian='False', entity='False', name='non-entity', cleanname='non-entity', corp_desg=None, state=None, postcode=None, lat=None, lon=None, sa2_code=None, sa2_name=None, lga_code=None, lga_name=None, gcc_name=None, elect_div=None, abn=None, acn=None)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skJi0YPnDgcm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "staging_ipgod101.createOrReplaceTempView('staging_ipgod101')\n",
        "staging_ipgod102.createOrReplaceTempView('staging_ipgod102')\n",
        "staging_abr.createOrReplaceTempView('staging_abr')\n",
        "\n",
        "\n",
        "application =  spark.sql(\"\"\" SELECT DISTINCT \n",
        "                                      staging_ipgod101.australian_appl_no,\n",
        "                                      staging_ipgod101.patent_status_type,\n",
        "                                      staging_ipgod101.patent_type,\n",
        "                                      staging_ipgod101.primary_ipc_mark_value,\n",
        "                                      staging_ipgod102.ipa_id,\n",
        "                                      staging_ipgod102.lat,\n",
        "                                      staging_ipgod102.lon\n",
        "                                      from staging_ipgod101 inner join staging_ipgod102\n",
        "                                      on staging_ipgod101.australian_appl_no = staging_ipgod102.australian_appl_no\n",
        "                                      where staging_ipgod102.australian = 'True' and staging_ipgod102.entity = 'True'\n",
        "                                      order by staging_ipgod101.australian_appl_no,staging_ipgod102.ipa_id\n",
        "                                      \"\"\")\n",
        "\n",
        "region = spark.sql(\"\"\"SELECT DISTINCT\n",
        "                                       lat,\n",
        "                                       lon,\n",
        "                                       state,\n",
        "                                       sa2_code,\n",
        "                                       sa2_name,\n",
        "                                       lga_code,\n",
        "                                       lga_name,\n",
        "                                       gcc_name,\n",
        "                                       elect_div\n",
        "                                   FROM staging_ipgod102\n",
        "                                   where australian = 'True' and entity = 'True' and lat is not NULL and lon is not NULL\n",
        "                                   order by lat,lon\n",
        "                                   \"\"\") \n",
        "\n",
        "business = spark.sql(\"\"\"SELECT DISTINCT\n",
        "                                  staging_ipgod102.ipa_id,\n",
        "                                  cast(staging_ipgod102.abn as int) as abn,\n",
        "                                  staging_ipgod102.cleanname,\n",
        "                                  staging_abr.BN_STATUS as business_registration_status,\n",
        "                                  staging_abr.BN_REG_DT as business_registration_date,\n",
        "                                  staging_abr.BN_REG_DT as business_cancel_date,\n",
        "                                  staging_abr.BN_RENEW_DT as business_renew_date\n",
        "                      from staging_ipgod102 left join staging_abr \n",
        "                                  on cast(staging_ipgod102.abn as int) = cast(staging_abr.BN_ABN as int)\n",
        "                                  \"\"\")\n",
        "\n",
        "business = spark.sql(\"\"\"SELECT DISTINCT *\n",
        "                                  from(select staging_ipgod102.ipa_id,\n",
        "                                              cast(cast(abn as double) as int) as abn,\n",
        "                                               staging_ipgod102.cleanname\n",
        "                                        where  where australian = 'True' and entity = 'True' and lat is not NULL and lon is not NULL)\n",
        ",\n",
        "                                  staging_abr.BN_STATUS as business_registration_status,\n",
        "                                  staging_abr.BN_REG_DT as business_registration_date,\n",
        "                                  staging_abr.BN_REG_DT as business_cancel_date,\n",
        "                                  staging_abr.BN_RENEW_DT as business_renew_date\n",
        "                      from staging_ipgod102 left join staging_abr \n",
        "                                  on  cast(abn as double) = staging_abr.BN_ABN\n",
        "                                  \"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CrOk1bKT4gW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "business1 = spark.sql(\"\"\"SELECT DISTINCT\n",
        "                                  staging_ipgod102.ipa_id,\n",
        "                                  cast(cast(abn as double) as int) as abn,\n",
        "                                  staging_ipgod102.acn,\n",
        "                                  staging_ipgod102.cleanname\n",
        "                                  from staging_ipgod102  \n",
        "                               where australian = 'True' and entity = 'True' and lat is not NULL and lon is not NULL\n",
        "\"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkOpmxafFSHu",
        "colab_type": "code",
        "outputId": "c97aad32-f0ae-4de8-ac9d-5f3ac7ecf554",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "business.head(50)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-117-cd181cc73e83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbusiness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mhead\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1251\u001b[0m             \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1253\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \"\"\"\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \"\"\"\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1756.collectToPython.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(ipa_id#398, abn#1390, cleanname#403, business_registration_status#1391, business_registration_date#1392, business_cancel_date#1393, business_renew_date#1394, 200)\n+- *(5) HashAggregate(keys=[ipa_id#398, abn#1390, cleanname#403, business_registration_status#1391, business_registration_date#1392, business_cancel_date#1393, business_renew_date#1394], functions=[], output=[ipa_id#398, abn#1390, cleanname#403, business_registration_status#1391, business_registration_date#1392, business_cancel_date#1393, business_renew_date#1394])\n   +- *(5) Project [ipa_id#398, cast(cast(abn#415 as double) as int) AS abn#1390, cleanname#403, BN_STATUS#120 AS business_registration_status#1391, BN_REG_DT#121 AS business_registration_date#1392, BN_REG_DT#121 AS business_cancel_date#1393, BN_RENEW_DT#123 AS business_renew_date#1394]\n      +- SortMergeJoin [cast(abn#415 as double)], [BN_ABN#126], LeftOuter\n         :- *(2) Sort [cast(abn#415 as double) ASC NULLS FIRST], false, 0\n         :  +- Exchange hashpartitioning(cast(abn#415 as double), 200)\n         :     +- *(1) Project [ipa_id#398, cleanname#403, abn#415]\n         :        +- *(1) Filter (((((isnotnull(australian#400) && isnotnull(entity#401)) && (australian#400 = True)) && (entity#401 = True)) && isnotnull(lat#407)) && isnotnull(lon#408))\n         :           +- *(1) FileScan csv [ipa_id#398,australian#400,entity#401,cleanname#403,lat#407,lon#408,abn#415] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/tmp/spark-f116acc2-3df6-4587-a7ab-edf2052b84d5/userFiles-44177276-2166-48..., PartitionFilters: [], PushedFilters: [IsNotNull(australian), IsNotNull(entity), EqualTo(australian,True), EqualTo(entity,True), IsNotN..., ReadSchema: struct<ipa_id:string,australian:string,entity:string,cleanname:string,lat:string,lon:string,abn:s...\n         +- *(4) Sort [BN_ABN#126 ASC NULLS FIRST], false, 0\n            +- Exchange hashpartitioning(BN_ABN#126, 200)\n               +- *(3) Project [BN_STATUS#120, BN_REG_DT#121, BN_RENEW_DT#123, BN_ABN#126]\n                  +- *(3) Filter isnotnull(BN_ABN#126)\n                     +- Scan ExistingRDD[REGISTER_NAME#118,BN_NAME#119,BN_STATUS#120,BN_REG_DT#121,BN_CANCEL_DT#122,BN_RENEW_DT#123,BN_STATE_NUM#124,BN_STATE_OF_REG#125,BN_ABN#126]\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:339)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3263)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3260)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3260)\n\tat jdk.internal.reflect.GeneratedMethodAccessor65.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: java.lang.IllegalArgumentException: Unsupported class file major version 55\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:166)\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:148)\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:136)\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:237)\n\tat org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:517)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:500)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:500)\n\tat org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)\n\tat org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)\n\tat org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)\n\tat org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:355)\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:307)\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:306)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:306)\n\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:162)\n\tat org.apache.spark.SparkContext.clean(SparkContext.scala:2326)\n\tat org.apache.spark.rdd.RDD$$anonfun$zipPartitions$1.apply(RDD.scala:886)\n\tat org.apache.spark.rdd.RDD$$anonfun$zipPartitions$1.apply(RDD.scala:886)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.zipPartitions(RDD.scala:885)\n\tat org.apache.spark.rdd.RDD$$anonfun$zipPartitions$2.apply(RDD.scala:892)\n\tat org.apache.spark.rdd.RDD$$anonfun$zipPartitions$2.apply(RDD.scala:892)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.zipPartitions(RDD.scala:891)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:150)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 37 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwqy64ORFRP7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNE6YiHtD3EM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "region_table =  region.select(\"start_time\").distinct() \\\n",
        "        .withColumn(\"hour\", hour(\"start_time\")).withColumn(\"day\", dayofmonth(\"start_time\")) \\\n",
        "        .withColumn(\"week\", weekofyear(\"start_time\")).withColumn(\"month\", month(\"start_time\")) \\\n",
        "        .withColumn(\"year\", year(\"start_time\")).withColumn(\"weekday\", dayofweek(\"start_time\"))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}